# -*- coding: utf-8 -*-
"""assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1srJlmT8x8EPW6OjzCTK0Ane9TosGAL0q
"""

# Amazon food review Dataset

!pip install sentence-transformers faiss-cpu transformers

# Step 1: Load Data from database.sqlite

import sqlite3
import pandas as pd

# Connect to the SQLite database
db_path = '/content/database.sqlite'
conn = sqlite3.connect(db_path)

# Check the tables in the SQLite database
tables = pd.read_sql_query("SELECT name FROM sqlite_master WHERE type='table';", conn)
print(tables)

# Load data from the 'Reviews' table (adjust table name if needed)
query = """
SELECT ProductId, UserId, Score, Summary, Text
FROM Reviews
WHERE Text IS NOT NULL;
"""

df_sqlite = pd.read_sql_query(query, conn)
print(df_sqlite.head())

# Close the connection
conn.close()

# Step 2: Load Data from reviews.csv

import pandas as pd

df_csv = pd.read_csv('/content/Reviews.csv')

# Preview the data
print(df_csv.head())

# Step 3: Data Preprocessing

import re

# Data Preprocessing: Clean the text data
def clean_text(text):
    # Remove punctuation and non-alphanumeric characters
    return re.sub(r'[^\w\s]', '', str(text))

# Clean the text from both dataframes
df_sqlite['Cleaned_Text'] = df_sqlite['Text'].apply(clean_text)
df_csv['Cleaned_Text'] = df_csv['Text'].apply(clean_text)

# Check cleaned data
print(df_sqlite[['Text', 'Cleaned_Text']].head())
print(df_csv[['Text', 'Cleaned_Text']].head())

# Step 4: Vectorization (Generate Embeddings)

from sentence_transformers import SentenceTransformer

# Load the pre-trained model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Generate embeddings for the cleaned text
embeddings_sqlite = model.encode(df_sqlite['Cleaned_Text'].tolist(), show_progress_bar=True)
embeddings_csv = model.encode(df_csv['Cleaned_Text'].tolist(), show_progress_bar=True)

# Save embeddings for future use
import numpy as np
np.save('/content/embeddings_sqlite.npy', embeddings_sqlite)
np.save('/content/embeddings_csv.npy', embeddings_csv)

# Step 5: Store Embeddings and Perform Query Retrieval Using FAISS

import faiss

# Create FAISS index for SQLite embeddings
d_sqlite = embeddings_sqlite.shape[1]  # Dimension of the embeddings
index_sqlite = faiss.IndexFlatL2(d_sqlite)  # L2 distance
index_sqlite.add(embeddings_sqlite)

# Create FAISS index for CSV embeddings
d_csv = embeddings_csv.shape[1]
index_csv = faiss.IndexFlatL2(d_csv)
index_csv.add(embeddings_csv)

# Save FAISS index
faiss.write_index(index_sqlite, "/content/faiss_index_sqlite")
faiss.write_index(index_csv, "/content/faiss_index_csv")

# Query the index (Example query text)
query_text = "Excellent customer service"
query_embedding = model.encode([query_text])

# Retrieve top 5 similar results from SQLite data
D_sqlite, I_sqlite = index_sqlite.search(query_embedding, k=5)

# Retrieve top 5 similar results from CSV data
D_csv, I_csv = index_csv.search(query_embedding, k=5)

# Display retrieved results
print("Top 5 Matches from SQLite:")
for idx in I_sqlite[0]:
    print(df_sqlite.iloc[idx]['Text'])

print("\nTop 5 Matches from CSV:")
for idx in I_csv[0]:
    print(df_csv.iloc[idx]['Text'])

# Step 6: Implement Retriever-Augmented Generation (RAG)
from transformers import pipeline

# Initialize a summarization model
summarizer = pipeline('summarization')

# Combine the retrieved text for SQLite and CSV
retrieved_sqlite_text = " ".join([df_sqlite.iloc[idx]['Text'] for idx in I_sqlite[0]])
retrieved_csv_text = " ".join([df_csv.iloc[idx]['Text'] for idx in I_csv[0]])

# Generate summaries for the retrieved text
summary_sqlite = summarizer(retrieved_sqlite_text, max_length=50, min_length=25, do_sample=False)
summary_csv = summarizer(retrieved_csv_text, max_length=50, min_length=25, do_sample=False)

print("\nGenerated Summary from SQLite Matches:")
print(summary_sqlite[0]['summary_text'])

print("\nGenerated Summary from CSV Matches:")
print(summary_csv[0]['summary_text'])

# Step 7: Save Processed Data and Embeddings

df_sqlite.to_csv('/content/processed_sqlite_reviews.csv', index=False)
df_csv.to_csv('/content/processed_csv_reviews.csv', index=False)

# Save embeddings as numpy arrays
np.save('/content/embeddings_sqlite.npy', embeddings_sqlite)
np.save('/content/embeddings_csv.npy', embeddings_csv)

# Step 8: Monitoring and Logging
import logging
import time

# Setting up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Log start of data ingestion
logging.info("Starting data ingestion from database.sqlite...")
start_time = time.time()

# Connect to SQLite and load data
conn = sqlite3.connect(db_path)
df_sqlite = pd.read_sql_query(query, conn)
conn.close()

logging.info(f"Data ingestion complete. Time taken: {time.time() - start_time} seconds")

# Log data preprocessing step
logging.info("Starting data preprocessing...")
start_time = time.time()

df_sqlite['Cleaned_Text'] = df_sqlite['Text'].apply(clean_text)

logging.info(f"Data preprocessing complete. Time taken: {time.time() - start_time} seconds")

# Log vectorization step
logging.info("Starting vectorization...")
start_time = time.time()

embeddings_sqlite = model.encode(df_sqlite['Cleaned_Text'].tolist(), show_progress_bar=True)

logging.info(f"Vectorization complete. Time taken: {time.time() - start_time} seconds")

# Log query retrieval step
logging.info("Starting query retrieval using FAISS...")
start_time = time.time()

D_sqlite, I_sqlite = index_sqlite.search(query_embedding, k=5)

logging.info(f"Query retrieval complete. Time taken: {time.time() - start_time} seconds")

# Log RAG summary generation
logging.info("Starting RAG summary generation...")
start_time = time.time()

summary_sqlite = summarizer(retrieved_sqlite_text, max_length=50, min_length=25, do_sample=False)

logging.info(f"RAG summary generation complete. Time taken: {time.time() - start_time} seconds")

# Bonus: Optimization for scalability

import numpy as np

# Function to batch process data
def batch_process_embeddings(data, batch_size=100):
    embeddings = []
    for i in range(0, len(data), batch_size):
        batch = data[i:i+batch_size]
        embeddings.extend(model.encode(batch, show_progress_bar=True))
    return np.array(embeddings)

# Apply batch processing for embeddings
embeddings_sqlite_batch = batch_process_embeddings(df_sqlite['Cleaned_Text'].tolist())
embeddings_csv_batch = batch_process_embeddings(df_csv['Cleaned_Text'].tolist())

# API creation
from fastapi import FastAPI
app = FastAPI()

@app.post("/query/")
def query_text(input_text: str):
    query_embedding = model.encode([input_text])
    D, I = index_sqlite.search(query_embedding, k=5)
    results = [df_sqlite.iloc[idx]['Text'] for idx in I[0]]
    return {"query": input_text, "results": results}

